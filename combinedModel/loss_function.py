# -*- coding: utf-8 -*-
"""loss_function.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16qamyHnP_FwbtTxoLR2lKCkohzEDA0bg
"""

import torch
import torch.fft

def mse_loss(x, y):
    """Calcul de la MSE classique."""
    return torch.mean((x - y) ** 2)

def compute_spectral_energy(field, ltilde):
    """
    Calcule l'énergie spectrale d'un champ (sur les dimensions horizontales)
    en effectuant une FFT 2D et en binant les coefficients selon leur distance radiale.

    field : tenseur de forme (..., lat, lon)
    ltilde : coupure spectrale (nombre entier)

    Retourne : tenseur de forme (..., ltilde+1) donnant, pour chaque bin radial,
    la somme de l'énergie des coefficients correspondants.
    """
    # Calcul de la FFT 2D sur les deux dernières dimensions (latitude, longitude)
    field_fft = torch.fft.rfft2(field)
    energy = torch.abs(field_fft) ** 2  # énergie par coefficient

    # Dimensions horizontales
    nlat = field.shape[-2]
    nlon_fft = field_fft.shape[-1]

    # Création d'une grille d'indices pour les fréquences (on considère ici les indices comme proxy des fréquences)
    y = torch.arange(nlat, device=field.device).float()
    x = torch.arange(nlon_fft, device=field.device).float()
    grid_y, grid_x = torch.meshgrid(y, x, indexing='ij')
    # Calcul de la "distance radiale" (approximative) de chaque coefficient
    radial = torch.sqrt(grid_y ** 2 + grid_x ** 2)

    # Aplatir les deux dernières dimensions
    energy_flat = energy.reshape(*energy.shape[:-2], -1)  # forme (..., nlat*nlon_fft)
    radial_flat = radial.reshape(-1)  # forme (nlat*nlon_fft)

    # Binning radial : pour r = 0 à ltilde, on somme les énergies des coefficients dont la distance radiale est dans [r-0.5, r+0.5)
    binned_energy = []
    for r in range(ltilde + 1):
        mask = (radial_flat >= (r - 0.5)) & (radial_flat < (r + 0.5))
        if mask.sum() > 0:
            # On somme sur la dernière dimension (les coefficients du bin)
            energy_r = energy_flat[..., mask]
            binned_sum = energy_r.sum(dim=-1)
        else:
            # S'il n'y a aucun coefficient dans ce bin, on remplit avec des zéros
            binned_sum = torch.zeros(energy.shape[:-2], device=field.device)
        binned_energy.append(binned_sum)
    # Concaténation des bins le long d'une nouvelle dimension
    binned_energy = torch.stack(binned_energy, dim=-1)
    return binned_energy  # forme (..., ltilde+1)

def spectral_mse(field_pred, field_true, ltilde):
    """
    Calcule la MSE entre les énergies spectrales de field_pred et field_true.

    field_pred, field_true : tenseurs de forme (..., lat, lon)
    """
    energy_pred = compute_spectral_energy(field_pred, ltilde)
    energy_true = compute_spectral_energy(field_true, ltilde)
    return torch.mean((energy_pred - energy_true) ** 2)

def spectral_bias_mse(field_pred, field_true, ltilde):
    """
    Calcule la perte de biais spectral :
    on moyenne d'abord (sur batch et lead_time) la différence,
    puis on calcule l'énergie spectrale du biais.

    field_pred, field_true : tenseurs de forme (batch, lead_time, ..., lat, lon)
    """
    # Moyenne sur les dimensions batch et lead_time (dimensions 0 et 1)
    diff_mean = torch.mean(field_pred - field_true, dim=(0, 1))
    # On compare ce biais moyen à zéro en termes d'énergie spectrale
    return spectral_mse(diff_mean, torch.zeros_like(diff_mean), ltilde)

def combined_loss(out_state, data_era5, ltilde=42):
    """
    Calcule la fonction de perte totale pour le modèle déterministe.

    out_state et data_era5 sont des dictionnaires contenant les prédictions
    et les observations respectivement, pour deux représentations :
      - "data" : par exemple sur niveaux de pression
      - "model" : par exemple sur niveaux sigma

    Chaque représentation est elle-même un dictionnaire avec les variables météorologiques
    (par exemple, "divergence", "vorticity", "temperature", etc.).

    La fonction de perte totale est définie par :
      L = λ_data * M_Data + λ_spec * M_DataSpec + λ_model * M_Model + λ_spec * M_ModelSpec + λ_bias * M_MSBias
    """
    # Coefficients de pondération
    lambda_data = 20.0
    lambda_spec = 0.1
    lambda_model = 1.0
    lambda_bias = 2.0

    # Initialisation des pertes
    loss_data = 0.0
    loss_data_spec = 0.0
    loss_model = 0.0
    loss_model_spec = 0.0
    loss_bias = 0.0

    # Liste des variables à prendre en compte
    variables = [
        'divergence',
        'vorticity',
        'temperature_deviation',
        'temperature',
        'specific_humidity',
        'specific_cloud_ice_water_content',
        'specific_cloud_liquid_water_content',
        'log_surface_pressure'
    ]

    """# On ajoute log_surface_pressure s'il est présent
    if 'log_surface_pressure' in out_state['data']:
        variables.append('log_surface_pressure')"""

    # Calcul des pertes pour chaque variable
    for var in variables:
        # Pertes sur la représentation "data" (ex. niveaux de pression)
        pred_data = out_state[var]   # forme : (batch, lead_time, level, lat, lon) #['data']
        true_data = data_era5[var]     # forme identique  #['data']
        loss_data += mse_loss(pred_data, true_data)
        loss_data_spec += spectral_mse(pred_data, true_data, ltilde)

        # Pertes sur la représentation "model" (ex. niveaux sigma)
        pred_model = out_state[var]   # forme : (batch, lead_time, sigma, lat, lon)               ['model']
        true_model = data_era5[var]     # forme identique                                         ['model']
        loss_model += mse_loss(pred_model, true_model)
        loss_model_spec += spectral_mse(pred_model, true_model, ltilde)

        # Perte de biais (calculée sur la représentation "data")
        loss_bias += spectral_bias_mse(pred_data, true_data, ltilde)

    # Moyennage sur le nombre de variables
    num_vars = len(variables)
    loss_data = loss_data / num_vars
    loss_data_spec = loss_data_spec / num_vars
    loss_model = loss_model / num_vars
    loss_model_spec = loss_model_spec / num_vars
    loss_bias = loss_bias / num_vars

    # Fonction de perte totale
    total_loss = (lambda_data * loss_data +
                  lambda_spec * loss_data_spec +
                  lambda_model * loss_model +
                  lambda_spec * loss_model_spec +
                  lambda_bias * loss_bias)

    return total_loss